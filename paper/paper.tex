\documentclass[conference,11pt]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage[verbose,expansion=alltext,stretch=50]{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\newcommand{\link}[1]{{\color{blue}\href{#1}{#1}}}

\title{DeepSched: A Deep Representation Of Scheduling Policies For Heterogeneous Distributed Systems}

\author{
\IEEEauthorblockN{Mohamed Shawky}
\IEEEauthorblockA{Computer Engineering\\
Cairo University\\
Email: mohamedshawky911@gmail.com} \\
\IEEEauthorblockN{Remonda Talaat}
\IEEEauthorblockA{Computer Engineering\\
Cairo University\\
Email: remondatalaat21@gmail.com}
\and
\IEEEauthorblockN{Mahmoud Adas}
\IEEEauthorblockA{Computer Engineering\\
Cairo University\\
Email: mido3ds@gmail.com} \\
\IEEEauthorblockN{Evram Youssef}
\IEEEauthorblockA{Computer Engineering\\
Cairo University\\
Email: evramyousef@gmail.com}
}

\begin{document}
\maketitle

\begin{abstract}
Task scheduling is one of the core problems in any distributed system. The ability of the system to assign tasks to its different resources to achieve the best possible performance with the least resource consumption is the main challenge in distributed systems research. The heterogeneity of the distributed systems adds another layer of complexity on top of the mentioned problem. The different execution time on different processors and different task dependencies make the scheduling problem very computationally intensive. Usually, researchers and developers turn to greedy-based methods or even recursive optimization methods to reach some optimal or sub-optimal solutions. However, due to the nature of the problem, these solutions might take very long time and usually degrade in complex situations. In this work, we propose the idea of scheduling algorithms approximation, where we approximate the complex scheduling algorithms using other models to achieve the same results but with lower execution time. Neural networks are proposed for this task, as they are great function approximators. The complexity of neural networks comes at training time, however they can maintain constant execution time in inference. This constant execution time is relatively low, as it's a simple feedforward process that highly supports parallelism. This makes neural networks a perfect candidate for us, however this comes at a cost of static scheduling scenarios, which is useful in some case such as simulation workflow on supercomputers. The proposed network can achieve almost the same performance of the approximated method (HEFT) on seen data during training and slightly worse performance on unseen data with constant execution time at different input sizes.
\end{abstract}

\section{Introduction}
\input{core/introduction}

\section{Related Work}
\input{core/related_work}

\section{Methodology}
\input{core/methodology}
    
\section{Experimental Setup}
\input{core/exp_setup}

\section{Discussion and Results}
\input{core/results}

\section{Conclusion and Future Work}
\input{core/conclusion}

\medskip

\bibliographystyle{unsrt}
\bibliography{paper}
    
\end{document}
